{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们不能在机器学习中直接使用文本数据，所以我们需要将其转换为数值向量。本文章将为你介绍所有的文本转换技术。\n",
    "\n",
    " \n",
    "\n",
    "> 在将文本数据提供给机器学习模型之前，需要将文本数据清洗并编码为数值，这种清洗和编码的过程称为**“文本预处理”**\n",
    "\n",
    " \n",
    "\n",
    "在本篇文章中，我们将看到一些基本的文本清洗步骤和文本数据编码技术。 我们将会看到：\n",
    "\n",
    "1. **了解数据** - 明白数据的全部含义。清理数据时应该考虑哪些事项（标点符号、停用词等）。\n",
    "\n",
    "2. **基本清理** - 我们将看到清理数据时需要考虑哪些参数（例如标点符号，停用词等）及其代码。\n",
    "\n",
    "3. 编码技术\n",
    "\n",
    "    \\- All the popular techniques that are used for encoding that I personally came across.\n",
    "\n",
    "   - **Bag of Words**（词袋模型）\n",
    "   - **Binary Bag of Words**（二进制词袋模型）\n",
    "   - **Bigram, Ngram**\n",
    "   - **TF-IDF**( **T**erm  **F**requency - **I**nverse **D**ocument **F**requency词频-逆文档频率)\n",
    "   - **Word2Vec**\n",
    "   - **Avg-Word2Vec**\n",
    "   - **TF-IDF Word2Vec**\n",
    "\n",
    "现在让我们开始这个有趣的旅程吧！\n",
    " \n",
    "\n",
    "**导入库**\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")         # 忽略不必要的警告\n",
    "\n",
    "import numpy as np                        # 用于大型和多维数组\n",
    "import pandas as pd                       # 用于数据处理和分析\n",
    "import nltk                               # 自然语言处理工具包\n",
    "\n",
    "from nltk.corpus import stopwords         # 停用词语料库\n",
    "from nltk.stem import PorterStemmer       # 词干提取器\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer    #For Bag of words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer    #For TF-IDF\n",
    "from gensim.models import Word2Vec                             #For Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./input/Reviews.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "data_sel = data.head(10000)                 # 只考虑前 10000 行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
      "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 查看数据的结构\n",
    "print(data_sel.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **了解数据**\n",
    "\n",
    "从数据集来看，我们的主要目标是根据文本预测评论是**正面**还是**负面**。\n",
    "\n",
    "我们看到Score列，它的的取值有1,2,3,4,5。其中1、2为负面评价，4、5为正面评价。对于得分=3，我们将其视为中性评价，让我们删除中性行，以便可以预测正或负。\n",
    "\n",
    "HelfulnessNumerator表示有多少人发现评论有用，HelpfulnessDenominator是关于usefull 评论数 + not so usefull 评论数的。由此我们可以看出，HelfulnessNumerator 总是小于或等于 HelpfulnessDenominator。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_score_removed = data_sel[data_sel['Score']!=3]       # 去除中性评价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "将分数值转换为类别标签Postiuve或Negative。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'positive'\n",
    "    return 'negative'\n",
    "\n",
    "score_upd = data_score_removed['Score']\n",
    "t = score_upd.map(partition)  # Series.map方法，接受一个函数或含有映射关系的字典型对象\n",
    "data_score_removed['Score']=t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **基本清洗**\n",
    "\n",
    "**重复数据删除**表示删除重复的行，必须删除重复的行才能获得稳定的结果。 根据UserId，ProfileName，Time，Text检查重复项。 如果所有这些值都相等，那么我们将删除这些记录。 （没有用户可以在相同的确切时间输入不同产品的评论。）\n",
    "\n",
    "我们已经知道HelpfulnessNumerator应该始终小于或等于HelpfulnessDenominator，因此检查此条件并删除这些记录。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = data_score_removed.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = final_data[final_data['HelpfulnessNumerator'] <= final_data['HelpfulnessDenominator']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_X = final['Text']\n",
    "final_y = final['Score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "将所有单词转换为小写并删除标点符号和html标签（如果有）。\n",
    "\n",
    "**词干提取**-将单词转换为基本词或词干（例如 - tastefully, tasty，这些单词将转换为词干“ tasti”）。 因为我们不考虑所有相似的词，所以这减小了向量维数。\n",
    "\n",
    "**停用词** - 停用词是不必要的词，即使它们被删除了，句子的语义也不会发生变化。\n",
    "\n",
    "例如 - This pasta is so tasty ==> pasta tasty    ( This , is, so 都是停用词，因此将它们删除)\n",
    "\n",
    "参加以下代码单元，可以查看所有停用词。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'re', 'which', 'against', 'don', \"won't\", 'ours', \"doesn't\", 'other', 'very', 'weren', 'down', 'both', 'while', 'own', 'yourself', 'these', 'but', 'at', 'haven', 'between', 'doesn', 'y', 'it', \"mightn't\", 'been', 'won', 'its', 'mightn', 'ourselves', 'when', 'how', \"wouldn't\", 'who', 'once', 'their', 'her', 'each', 'then', 'for', 'you', \"mustn't\", 'so', 'am', \"it's\", 'here', 'wouldn', 'most', 'by', 'this', \"that'll\", 'what', 'being', 'off', 've', 'before', \"aren't\", \"she's\", 'shouldn', 'are', 'or', 'during', 'same', 'they', 'that', 'll', 'such', 'hadn', 'hasn', 'she', \"couldn't\", 'any', 'i', 'from', 'is', 'hers', 'after', \"hasn't\", 'were', 'over', \"wasn't\", 'more', \"you've\", 'he', 'with', 'wasn', 'nor', 'd', 'further', 'shan', 'out', 'them', 'on', \"shouldn't\", 'now', 'whom', 'as', \"you'd\", \"you'll\", 'because', 'all', 'too', 'my', 'no', \"didn't\", 'if', 'into', \"weren't\", 'does', 'than', 'mustn', 'himself', 'why', 'those', 'only', 'theirs', 'yours', 'couldn', 'up', 'me', \"needn't\", 'has', 'there', 'didn', 'above', 'needn', 'to', 'again', 'and', 'doing', 'the', 'did', 'until', 'aren', 'an', 'where', 'will', 'him', 'o', 'themselves', 'your', 'of', \"isn't\", 'myself', 'just', 'herself', 'can', 't', 'our', 'ain', 'we', 'a', 'under', \"don't\", \"you're\", 'be', \"should've\", 'isn', 'below', 'not', 'do', \"haven't\", 'should', 'had', 'having', 'his', 'ma', \"shan't\", 'through', 'yourselves', 'about', 's', 'have', 'was', 'few', 'some', \"hadn't\", 'itself', 'in', 'm'}\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english')) \n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "temp =[]\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "for sentence in final_X:\n",
    "    sentence = sentence.lower()                 # 转换为小写\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    sentence = re.sub(cleanr, ' ', sentence)        #将 HTML 标签替换为空格\n",
    "    sentence = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #将标点符号替换为空格\n",
    "    \n",
    "    words = [snow.stem(word) for word in sentence.split() if word not in stopwords.words('english')]   # 删除停用词并提取词干\n",
    "    temp.append(words)\n",
    "    \n",
    "final_X = temp   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product', 'arriv', 'label', 'jumbo', 'salt', 'peanut', 'peanut', 'actual', 'small', 'size', 'unsalt', 'sure', 'error', 'vendor', 'intend', 'repres', 'product', 'jumbo']\n"
     ]
    }
   ],
   "source": [
    "print(final_X[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " product arriv label jumbo salt peanut peanut actual small size unsalt sure error vendor intend repres product jumbo\n"
     ]
    }
   ],
   "source": [
    "sent = []\n",
    "for row in final_X:\n",
    "    sequ = ''\n",
    "    for word in row:\n",
    "        sequ = sequ + ' ' + word\n",
    "    sent.append(sequ)\n",
    "\n",
    "final_X = sent\n",
    "print(final_X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **编码技术**\n",
    "\n",
    "   **词袋模型**\n",
    "\n",
    "   在BoW中，我们构建了一个词典，其中包含来自文本评论数据集中的所有不重复单词的集合。在这里会计算每一个单词出现的频率。 如果字典中有 **d** 个不重复词，则对于每个句子或评论，其向量的长度将为 **d**，并将来自评论的词数存储在向量中的特定位置。 在这种情况下，向量将会非常稀疏。\n",
    "\n",
    "   例如. pasta is tasty and pasta is good\n",
    "\n",
    "   **[0]....[1]............[1]...........[2]..........[2]............[1]..........**            <== 它的向量表示（其余所有点将表示为零）\n",
    "\n",
    "   **[a]..[and].....[good].......[is].......[pasta]....[tasty].......**            <== 这是词典\n",
    "\n",
    "   使用scikit-learn的CountVectorizer，我们可以获取BoW并检查其包含的所有参数，其中之一是max_features = 5000，它表示仅考虑将前5000个最频繁重复的单词放置在字典中。 因此我们的字典长度或向量长度只有5000。\n",
    "\n",
    "**二进制词袋模型**\n",
    "\n",
    "在二进制BoW中，我们不计算单词出现的频率，如果单词出现在评论中，我们就只将对应位置设置为 **1**，否则设置为 **0**。 在CountVectorizer中，有一个参数 **binary = true**，它可以使我们的BoW变为二进制BoW。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3641)\t1\n",
      "  (0, 2326)\t1\n",
      "  (0, 4734)\t1\n",
      "  (0, 1539)\t1\n",
      "  (0, 4314)\t1\n",
      "  (0, 4676)\t1\n",
      "  (0, 3980)\t1\n",
      "  (0, 4013)\t1\n",
      "  (0, 162)\t1\n",
      "  (0, 3219)\t2\n",
      "  (0, 3770)\t1\n",
      "  (0, 2420)\t2\n",
      "  (0, 2493)\t1\n",
      "  (0, 332)\t1\n",
      "  (0, 3432)\t2\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(max_features=5000)\n",
    "bow_data = count_vect.fit_transform(final_X)  # 稀疏矩阵\n",
    "print(bow_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**词袋/二进制词袋模型的缺点**\n",
    "\n",
    "我们在进行这些文本到矢量编码时的主要目标是，意思相近的文本矢量应该彼此接近，但是在某些情况下，这对于Bow来说是不可能的\n",
    "\n",
    "例如，如果我们考虑两个评论 “ **This pasta is very tasty**” 和 “**This pasta is not tasty** ”，则在停用词移除后，这两个句子都将转换为 “ **pasta tasty**”，因此两者的含义完全相同。\n",
    "\n",
    "主要问题是这里我们没有考虑与每个单词相关的前后词，于是就有了Bigram和Ngram技术。\n",
    "\n",
    " \n",
    "\n",
    "**BI-GRAM 词袋模型**\n",
    "\n",
    "考虑到用于创建字典的单词对为Bi-Gram，因此Tri-Gram表示三个连续的单词，以此类推到NGram。\n",
    "\n",
    "CountVectorizer 有一个参数 **ngram range** 如果分配给（1,2），表示为 Bi-Gram BoW。\n",
    "\n",
    "但这极大地增加了我们的字典容量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_B_X = final_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1, 207785)\n",
      "  (0, 143171)\t1\n",
      "  (0, 151696)\t1\n",
      "  (0, 95087)\t1\n",
      "  (0, 196648)\t1\n",
      "  (0, 60866)\t1\n",
      "  (0, 177168)\t1\n",
      "  (0, 193567)\t1\n",
      "  (0, 164722)\t1\n",
      "  (0, 165627)\t1\n",
      "  (0, 4021)\t1\n",
      "  (0, 133855)\t1\n",
      "  (0, 133898)\t1\n",
      "  (0, 155987)\t1\n",
      "  (0, 97865)\t1\n",
      "  (0, 100490)\t1\n",
      "  (0, 11861)\t1\n",
      "  (0, 142800)\t1\n",
      "  (0, 151689)\t1\n",
      "  (0, 95076)\t1\n",
      "  (0, 196632)\t1\n",
      "  (0, 60852)\t1\n",
      "  (0, 177092)\t1\n",
      "  (0, 193558)\t1\n",
      "  (0, 164485)\t1\n",
      "  (0, 165423)\t1\n",
      "  (0, 3831)\t1\n",
      "  (0, 133854)\t2\n",
      "  (0, 155850)\t1\n",
      "  (0, 97859)\t2\n",
      "  (0, 100430)\t1\n",
      "  (0, 11784)\t1\n",
      "  (0, 142748)\t2\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "Bigram_data = count_vect.fit_transform(final_B_X)\n",
    "print('Shape:', Bigram_data[1].shape)\n",
    "print(Bigram_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**词频 - 逆文档频率（TF-IDF）**\n",
    "\n",
    "**词频 - 逆文档频率**，它可以确保对最频繁使用的单词给予较少的重视，同时也考虑低频使用的单词。\n",
    "\n",
    "**词频**是在评论中出现 **特定单词（W）**的次数除以评论中的总单词数量 **（Wr）**。 词频值的范围是0到1。\n",
    "\n",
    "**逆文档频率 **的计算为 **log（文档总数（N）/包含特定单词的文档数（n））**。 这里的文档称为 “评论”。\n",
    "\n",
    "**词频 - 逆文档频率(TF-IDF)** 等于 $TF*IDF$ 即 $\\dfrac{W*log{\\frac Nn}}{Wr}$ \n",
    "\n",
    "使用scikit-learn的tfidfVectorizer，我们可以获得TF-IDF。\n",
    "\n",
    "那么，即使在这里，我们为每个单词都获得了TF-IDF值，在某些情况下，删除停用词后，它还是可能会将不同含义的评论视为是相似的。 因此，我们可以使用BI-Gram或NGram模型。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (8718, 5000)\n",
      "  (0, 3432)\t0.1822092004981035\n",
      "  (0, 332)\t0.1574317775964303\n",
      "  (0, 2493)\t0.18769649750089953\n",
      "  (0, 2420)\t0.5671119742041831\n",
      "  (0, 3770)\t0.1536626385509959\n",
      "  (0, 3219)\t0.3726548417697838\n",
      "  (0, 162)\t0.14731616688674187\n",
      "  (0, 4013)\t0.14731616688674187\n",
      "  (0, 3980)\t0.14758995053747803\n",
      "  (0, 4676)\t0.2703170210936338\n",
      "  (0, 4314)\t0.14376924933112933\n",
      "  (0, 1539)\t0.2676489579732629\n",
      "  (0, 4734)\t0.22110622670603633\n",
      "  (0, 2326)\t0.25860104128863787\n",
      "  (0, 3641)\t0.27633136515735446\n"
     ]
    }
   ],
   "source": [
    "final_tf = final_X\n",
    "tf_idf = TfidfVectorizer(max_features=5000)\n",
    "tf_data = tf_idf.fit_transform(final_tf)\n",
    "print('Shape:', tf_data.shape)\n",
    "print(tf_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "因此，要真正克服距离较近的语义评论的问题，我们需要使用Word2Vec。\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word2Vec实际上采用了单词的语义含义以及它们与其他单词之间的关系。 它学习单词之间的所有内部关系，以密集的向量形式表示单词。\n",
    "\n",
    "使用 **Gensim** 的库，我们就可以调用 Word2Vec，它接收的是像 **min_count = 5** 之类的参数，表示仅当单词在整个数据中重复超过5次时才会考虑。 **size = 50** 表示矢量长度为50，而 **workers** 表示的是运行此代码的核数。\n",
    "\n",
    "**Average Word2Vec**\n",
    "\n",
    "计算每个单词的Word2vec，将每个单词的向量相加，然后将向量除以句子的单词数，简单地求出每条评论中所有单词的Word2vec的平均值。\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_data = final_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = []\n",
    "for row in w2v_data: \n",
    "    splitted.append([word for word in row.split()])     #分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_w2v = Word2Vec(splitted,min_count=5,size=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_data = []\n",
    "for row in splitted:\n",
    "    vec = np.zeros(50)\n",
    "    count = 0\n",
    "    for word in row:\n",
    "        try:\n",
    "            vec += train_w2v[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    avg_data.append(vec/count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.45812476  0.08458711  0.05092981  0.14410483 -0.49741123 -0.20359915\n",
      "  0.09425076  0.31108052 -0.39218421  0.13923831 -0.03049097 -0.11326336\n",
      " -0.15359964  0.15278533 -0.12085474 -0.09121271  0.10288595 -0.28526957\n",
      " -0.22663371 -0.41127726  0.00271141  0.42055635 -0.23583491 -0.12041335\n",
      " -0.25532713 -0.12806206 -0.16156811 -0.57879642 -0.29231823  0.29289352\n",
      "  0.07047758 -0.05602346  0.15903048  0.00325343  0.01416281 -0.26845292\n",
      "  0.24614703 -0.32351884 -0.37319936 -0.05872503 -0.2490074  -0.3033209\n",
      " -0.75307091  0.19496457 -0.22516627  0.02649581  0.14338367  0.67597507\n",
      " -0.64155409  0.07120903]\n"
     ]
    }
   ],
   "source": [
    "print(avg_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**TF-IDF WORD2VEC**\n",
    "\n",
    "在TF-IDF Word2Vec中，每个单词的Word2Vec值乘以该单词的tfidf值，然后求和，然后除以句子的tfidf值之和。\n",
    "\n",
    "就像是：\n",
    "\n",
    "```python\n",
    "                    V = ( t(W1)*w2v(W1) + t(W2)*w2v(W2) +.....+t(Wn)*w2v(Wn))/(t(W1)+t(W2)+....+t(Wn))\n",
    "```\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_w_data = final_X\n",
    "tf_idf = TfidfVectorizer()\n",
    "# tf_idf = TfidfVectorizer(max_features=5000)\n",
    "tf_idf_data = tf_idf.fit_transform(tf_w_data)\n",
    "# print(tf_idf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.65244400e-01  4.57776099e-02  9.98857835e-02  1.97531826e-01\n",
      " -4.82579953e-01 -1.04751093e-01  4.78762299e-02  2.98722871e-01\n",
      " -4.82290494e-01  9.07129827e-02 -2.50973641e-01 -2.97829000e-01\n",
      " -5.46013241e-02  1.49918537e-01 -3.34651424e-01 -1.32334573e-01\n",
      "  4.21436157e-02 -3.42838623e-01 -6.32972503e-03 -2.94089193e-01\n",
      "  1.89783762e-01  6.10301421e-01 -3.90488853e-01 -1.11587678e-01\n",
      " -1.86647763e-01 -2.65755767e-01 -6.99710703e-02 -5.71435860e-01\n",
      " -1.34036573e-01  5.06373095e-01  1.20161662e-01 -2.63341359e-01\n",
      "  2.89655554e-02  6.42477261e-02  2.50360920e-01 -2.98312006e-01\n",
      "  3.39811540e-01 -4.62875199e-01 -3.81373302e-01 -2.46285669e-01\n",
      " -4.59487886e-01 -4.13959972e-01 -9.44517293e-01  3.63023605e-01\n",
      " -4.07729702e-01 -2.93615766e-02  1.58460594e-01  8.06446577e-01\n",
      " -7.49817592e-01  1.73608258e-04]\n"
     ]
    }
   ],
   "source": [
    "tf_w_data = []\n",
    "tf_idf_data = tf_idf_data.toarray()\n",
    "i = 0\n",
    "for row in splitted:\n",
    "    vec = [0 for i in range(50)]\n",
    "    \n",
    "    temp_tfidf = []\n",
    "    for val in tf_idf_data[i]:\n",
    "        if val != 0:\n",
    "            temp_tfidf.append(val)\n",
    "    \n",
    "    count = 0\n",
    "    tf_idf_sum = 0\n",
    "    for word in row:  # 注释: 此处如何确保tf-idf的单词顺序与评论中的单词顺序一一对应???\n",
    "        try:\n",
    "            count += 1\n",
    "            tf_idf_sum = tf_idf_sum + temp_tfidf[count-1]\n",
    "            vec += (temp_tfidf[count-1] * train_w2v[word])  # 不能保证是同一个单词的tfidf值和Word2Vec值\n",
    "        except:\n",
    "            pass\n",
    "    vec = (float)(1/tf_idf_sum) * vec\n",
    "    tf_w_data.append(vec)\n",
    "    i = i + 1\n",
    "\n",
    "print(tf_w_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "**结论**\n",
    "\n",
    "通过本文我们看到了将文本数据编码为数值向量的不同技术。 但是哪种技术适合我们的机器学习模型还要取决于数据的结构和模型的目标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
